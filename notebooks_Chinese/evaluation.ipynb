{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68e21aa1",
   "metadata": {},
   "source": [
    "# Evaluating Agents-2\n",
    "\n",
    "We have an email assistant that uses a router to triage emails and then passes the email to the agent for response generation. How can we be sure that it will work well in production? This is why testing is important: it guides our decisions about our agent architecture with quantifiable metrics like response quality, token usage, latency, or triage accuracy. [LangSmith](https://docs.smith.langchain.com/) offers two primary ways to test agents. \n",
    "\n",
    "![overview-img](img/overview_eval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9cf143",
   "metadata": {},
   "source": [
    "LangSmithæä¾›ä¸¤ç§è¯„ä¼°æ–¹å¼ï¼š\n",
    "\n",
    "1ã€ä¸ç°æœ‰çš„è¯„ä¼°æ¡†æ¶é›†æˆï¼šPytest/Vitest\n",
    "\n",
    "2ã€ä½¿ç”¨LangSmithè‡ªå¸¦çš„evaluate()\n",
    "\n",
    "ä¸‹é¢å°†ä»ä¸¤ä¸ªæ–¹é¢è¿›è¡Œè¯„ä¼°ï¼š\n",
    "\n",
    "1ã€å•å…ƒæµ‹è¯•ï¼š\n",
    "\n",
    "    ä½¿ç”¨Pytestå¯¹å·¥å…·è°ƒç”¨è¿›è¡Œè¯„ä¼°å’Œä½¿ç”¨evaluate()å¯¹triageè¿›è¡Œè¯„ä¼°\n",
    "\n",
    "2ã€ç«¯åˆ°ç«¯æµ‹è¯•ï¼š\n",
    "\n",
    "    ä½¿ç”¨Pytestè¯„ä¼°æ¡†æ¶ï¼Œè°ƒç”¨LLMå¯¹E-mail Outputè¿›è¡Œè¯„åˆ†ï¼Œå¹¶ç»™å‡ºè¯„åˆ†ç†ç”±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7f7048",
   "metadata": {},
   "source": [
    "#### Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c47d4c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2005c34d",
   "metadata": {},
   "source": [
    "## How to run Evaluations\n",
    "\n",
    "#### Pytest / Vitest\n",
    "\n",
    "[Pytest](https://docs.pytest.org/en/stable/) and Vitest are well known to many developers as a powerful tools for writing tests within the Python and JavaScript ecosystems. LangSmith integrates with these frameworks to allow you to write and run tests that log results to LangSmith. For this notebook, we'll use Pytest.\n",
    "* Pytest is a great way to get started for developers who are already familiar with their framework. \n",
    "* Pytest is great for more complex evaluations, where each agent test case requires specific checks and success criteria that are harder to generalize.\n",
    "\n",
    "#### LangSmith Datasets \n",
    "\n",
    "You can also create a dataset [in LangSmith](https://docs.smith.langchain.com/evaluation) and run our assistant against the dataset using the LangSmith evaluate API.\n",
    "* LangSmith datasets are great for teams who are collaboratively building out their test suite. \n",
    "* You can leverage production traces, annotation queues, synthetic data generation, and more, to add examples to an ever-growing golden dataset.\n",
    "* LangSmith datasets are great when you can define evaluators that can be applied to every test case in the dataset (ex. similarity, exact match accuracy, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b7c989",
   "metadata": {},
   "source": [
    "## Test Cases\n",
    "\n",
    "Testing often starts with defining the test cases, which can be a challenging process. In this case, we'll just define a set of example emails we want to handle along with a few things to test. You can see the test cases in `eval/email_dataset.py`, which contains the following:\n",
    "\n",
    "1. **Input Emails**: A collection of diverse email examples\n",
    "2. **Ground Truth Classifications**: `Respond`, `Notify`, `Ignore`\n",
    "3. **Expected Tool Calls**: Tools called for each email that requires a response\n",
    "4. **Response Criteria**: What makes a good response for emails requiring replies\n",
    "\n",
    "Note that we have both\n",
    "- End to end \"integration\" tests (e.g. Input Emails -> Agent -> Final Output vs Response Criteria)\n",
    "- Tests for specific steps in our workflow (e.g. Input Emails -> Agent -> Classification vs Ground Truth Classification)\n",
    "\n",
    "å·²ç»å‡†å¤‡å¥½äº†æµ‹è¯•é‚®ä»¶å’Œç­”æ¡ˆï¼Œæ—¢ä¼šè·‘å…¨æµç¨‹ï¼Œä¹Ÿä¼šå•ç‹¬æŠ½æŸ¥æ¯ä¸€æ­¥ï¼Œç¡®ä¿é‚®ä»¶åŠ©æ‰‹æ—¢åˆ†å¾—å‡†ï¼Œä¹Ÿå›å¾—å¥½ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8fdc2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email Input: {'author': 'Alice Smith <alice.smith@company.com>', 'to': 'Lance Martin <lance@company.com>', 'subject': 'Quick question about API documentation', 'email_thread': \"Hi Lance,\\n\\nI was reviewing the API documentation for the new authentication service and noticed a few endpoints seem to be missing from the specs. Could you help clarify if this was intentional or if we should update the docs?\\n\\nSpecifically, I'm looking at:\\n- /auth/refresh\\n- /auth/validate\\n\\nThanks!\\nAlice\"}\n",
      "Expected Triage Output: respond\n",
      "Expected Tool Calls: ['write_email', 'done']\n",
      "Response Criteria: \n",
      "â€¢ Send email with write_email tool call to acknowledge the question and confirm it will be investigated  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from email_assistant.eval.email_dataset import email_inputs, expected_tool_calls, triage_outputs_list, response_criteria_list\n",
    "\n",
    "test_case_ix = 0\n",
    "\n",
    "print(\"Email Input:\", email_inputs[test_case_ix])\n",
    "print(\"Expected Triage Output:\", triage_outputs_list[test_case_ix])\n",
    "print(\"Expected Tool Calls:\", expected_tool_calls[test_case_ix])\n",
    "print(\"Response Criteria:\", response_criteria_list[test_case_ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2337bd7c",
   "metadata": {},
   "source": [
    "## Pytest Example\n",
    "\n",
    "Let's take a look at how we can write a test for a specific part of our workflow with Pytest. We will test whether our `email_assistant` makes the right tool calls when responding to the emails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb888d",
   "metadata": {},
   "source": [
    "å½“ç»™å®šä¸€å°æµ‹è¯•é‚®ä»¶æ—¶ï¼ŒåŠ©æ‰‹æ˜¯å¦è°ƒç”¨äº†æˆ‘ä»¬æœŸæœ›çš„æ‰€æœ‰å·¥å…·ï¼ˆtool callsï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae92fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from email_assistant.eval.email_dataset import email_inputs, expected_tool_calls\n",
    "from email_assistant.utils import format_messages_string\n",
    "from email_assistant.email_assistant import email_assistant\n",
    "from email_assistant.utils import extract_tool_calls\n",
    "\n",
    "from langsmith import testing as t\n",
    "\n",
    "\n",
    "#åªæŒ‘äº†ç¬¬ 0 å’Œç¬¬ 3 å°é‚®ä»¶ä½œä¸ºç¤ºä¾‹è·‘æµ‹è¯•ã€‚\n",
    "@pytest.mark.langsmith\n",
    "@pytest.mark.parametrize(\n",
    "    \"email_input, expected_calls\",\n",
    "    [   # Pick some examples with e-mail reply expected\n",
    "        (email_inputs[0],expected_tool_calls[0]),\n",
    "        (email_inputs[3],expected_tool_calls[3]),\n",
    "    ],\n",
    ")\n",
    "def test_email_dataset_tool_calls(email_input, expected_calls):\n",
    "    \"\"\"Test if email processing contains expected tool calls.\n",
    "    \n",
    "    This test confirms that all expected tools are called during email processing,\n",
    "    but does not check the order of tool invocations or the number of invocations\n",
    "    per tool. Additional checks for these aspects could be added if desired.\n",
    "    \"\"\"\n",
    "    # Run the email assistant\n",
    "    messages = [{\"role\": \"user\", \"content\": str(email_input)}]\n",
    "    result = email_assistant.invoke({\"messages\": messages})\n",
    "#ç”¨ email_assistant.invoke(...) çœŸæ­£è·‘ä¸€éå®Œæ•´æµç¨‹ã€‚\n",
    "\n",
    "#ä»è¿”å›çš„å¯¹è¯å†å²ä¸­æŠ½å–å‡ºå®é™…å‘ç”Ÿçš„å·¥å…·è°ƒç”¨ã€‚\n",
    "    # Extract tool calls from messages list\n",
    "    extracted_tool_calls = extract_tool_calls(result['messages'])\n",
    "            \n",
    "    # Check if all expected tool calls are in the extracted ones\n",
    "    missing_calls = [call for call in expected_calls if call.lower() not in extracted_tool_calls]\n",
    "    \n",
    "   # æŠŠç¼ºå¤±é¡¹ã€å®é™…å·¥å…·ã€å®Œæ•´å¯¹è¯è®°å½•åˆ° LangSmithï¼Œæ–¹ä¾¿å¯è§†åŒ–è°ƒè¯•ã€‚\n",
    "    t.log_outputs({\n",
    "                \"missing_calls\": missing_calls,\n",
    "                \"extracted_tool_calls\": extracted_tool_calls,\n",
    "                \"response\": format_messages_string(result['messages'])\n",
    "            })\n",
    "\n",
    "    # Test passes if no expected calls are missing\n",
    "    assert len(missing_calls) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcbc980",
   "metadata": {},
   "source": [
    "åªéœ€ä¸€ä¸ªè£…é¥°å™¨å°±èƒ½è®©æµ‹è¯•ç»“æœè¢« LangSmith è®°å½•\n",
    "\n",
    "ç»™æµ‹è¯•å‡½æ•°åŠ ä¸Š @pytest.mark.langsmithï¼Œå¹¶æŠŠå®ƒæ”¾åˆ° notebooks/test_tools.pyï¼ˆæˆ–å…¶ä»–ä»»æ„æµ‹è¯•æ–‡ä»¶ï¼‰é‡Œå³å¯ã€‚è¿è¡Œæ—¶è¿™æ¡æµ‹è¯•ä¼šè‡ªåŠ¨åŒæ­¥æˆ LangSmith æ•°æ®é›†é‡Œçš„ä¸€æ¡ç¤ºä¾‹ï¼Œå¹¶ç”Ÿæˆå¯¹åº”çš„å®éªŒè®°å½•ã€‚\n",
    "\n",
    "ç”¨ @pytest.mark.parametrize æŠŠå¤§é‡æ•°æ®é›†ç¤ºä¾‹å–‚ç»™åŒä¸€ä¸ªæµ‹è¯•å‡½æ•°\n",
    "\n",
    "å’Œå¹³æ—¶å†™ pytest ä¸€æ ·ï¼Œç”¨ @pytest.mark.parametrize æŠŠå¤šç»„â€œè¾“å…¥ + æœŸæœ›è¾“å‡ºâ€ä¼ è¿›å»ï¼Œæ¯ç»„å‚æ•°éƒ½ä¼šå˜æˆæ•°æ®é›†é‡Œçš„ç‹¬ç«‹ç¤ºä¾‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700aba2a",
   "metadata": {},
   "source": [
    "You'll notice a few things. \n",
    "- To [run with Pytest and log test results to LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest), we only need to add the `@pytest.mark.langsmith ` decorator to our function and place it in a file, as you see in `notebooks/test_tools.py`. This will log the test results to LangSmith.\n",
    "- Second, we can pass dataset examples to the test function as shown [here](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#parametrize-with-pytestmarkparametrize) via `@pytest.mark.parametrize`. \n",
    "\n",
    "#### Running Pytest\n",
    "We can run the test from the command line. We've defined the above code in a python file. From the project root, run:\n",
    "\n",
    "`! LANGSMITH_TEST_SUITE='Email assistant: Test Tools For Interrupt'  pytest notebooks/test_tools.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53165e98",
   "metadata": {},
   "source": [
    "#### Viewing Experiment Result\n",
    "\n",
    "We can view the results in the LangSmith UI. The `assert len(missing_calls) == 0` is logged to the `Pass` column in LangSmith. The `log_outputs` are passed to the `Outputs` column and function arguments are passed to the `Inputs` column. Each input passed in `@pytest.mark.parametrize(` is a separate row logged to the `LANGSMITH_TEST_SUITE` project name in LangSmith, which is found under `Datasets & Experiments`.\n",
    "\n",
    "![Test Results](img/test_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd325e27",
   "metadata": {},
   "source": [
    "## LangSmith Datasets Example\n",
    "\n",
    "![overview-img](img/eval_detail.png)\n",
    "\n",
    "Let's take a look at how we can run evaluations with LangSmith datasets. In the previous example with Pytest, we evaluated the tool calling accuracy of the email assistant. Now, the dataset that we're going to evaluate here is specifically for the triage step of the email assistant, in classifying whether an email requires a response.\n",
    "\n",
    "#### Dataset Definition \n",
    "\n",
    "We can [create a dataset in LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#create-a-dataset) with the LangSmith SDK. The below code creates a dataset with the test cases in the `eval/email_dataset.py` file.\n",
    "\n",
    "ä½¿ç”¨ LangSmith å¹³å°åˆ›å»ºä¸€ä¸ªæ•°æ®é›†ï¼Œå¹¶å¡«å……æµ‹è¯•ç”¨ä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ea997ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "from email_assistant.eval.email_dataset import examples_triage\n",
    "\n",
    "# Initialize LangSmith client\n",
    "client = Client()\n",
    "\n",
    "# Dataset name\n",
    "dataset_name = \"E-mail Triage Evaluation\"\n",
    "\n",
    "# Create dataset if it doesn't exist\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name, \n",
    "        description=\"A dataset of e-mails and their triage decisions.\"\n",
    "    )\n",
    "    # Add examples to the dataset\n",
    "    client.create_examples(dataset_id=dataset.id, examples=examples_triage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3c7ecd",
   "metadata": {},
   "source": [
    "evaluate API çš„å®ç°åœ¨ langsmith.Client ç±»ä¸­"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2df606",
   "metadata": {},
   "source": [
    "#### Target Function\n",
    "\n",
    "The dataset has the following structure, with an e-mail input and a ground truth triage classification for the e-mail as output:\n",
    "\n",
    "```\n",
    "examples_triage = [\n",
    "  {\n",
    "      \"inputs\": {\"email_input\": email_input_1},\n",
    "      \"outputs\": {\"classification\": triage_output_1},   # NOTE: This becomes the reference_output in the created dataset\n",
    "  }, ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7d7e83f-3006-4386-9230-786545c7b1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Example Input (inputs): {'email_input': {'author': 'Alice Smith <alice.smith@company.com>', 'to': 'Lance Martin <lance@company.com>', 'subject': 'Quick question about API documentation', 'email_thread': \"Hi Lance,\\n\\nI was reviewing the API documentation for the new authentication service and noticed a few endpoints seem to be missing from the specs. Could you help clarify if this was intentional or if we should update the docs?\\n\\nSpecifically, I'm looking at:\\n- /auth/refresh\\n- /auth/validate\\n\\nThanks!\\nAlice\"}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Example Input (inputs):\", examples_triage[0]['inputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f292f070-7af6-4370-9338-e90bfd6b3d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Example Reference Output (reference_outputs): {'classification': 'respond'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Example Reference Output (reference_outputs):\", examples_triage[0]['outputs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8290e820",
   "metadata": {},
   "source": [
    "We define a function that takes the dataset inputs and passes them to our email assistant. The LangSmith [evaluate API](https://docs.smith.langchain.com/evaluation) passes the `inputs` dict to this function. This function then returns a dict with the agent's output. Because we are evaluating the triage step, we only need to return the classification decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdce365",
   "metadata": {},
   "source": [
    "evaluate API çš„å·¥ä½œæµç¨‹\n",
    "\n",
    "å½“ä½ è°ƒç”¨ client.evaluate() æ—¶ï¼Œå®ƒä¼šå¯åŠ¨ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„å®éªŒï¼Œæ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š\n",
    "\n",
    "evaluate API ä¼šéå†ä½ çš„æ•°æ®é›†ä¸­æ¯ä¸€ä¸ªè¾“å…¥ã€‚\n",
    "\n",
    "å®ƒå°†è¿™äº›è¾“å…¥ä¼ é€’ç»™ä½ çš„ç›®æ ‡å‡½æ•°ï¼Œè®©ä½ çš„ä»£ç ï¼ˆæ¯”å¦‚é‚®ä»¶åŠ©æ‰‹ï¼‰è¿è¡Œå¹¶ç”Ÿæˆä¸€ä¸ªå®é™…è¾“å‡ºã€‚\n",
    "\n",
    "ç„¶åï¼Œå®ƒå°†è¿™ä¸ªå®é™…è¾“å‡ºå’Œæ•°æ®é›†ä¸­çš„å‚è€ƒè¾“å‡ºä¸€èµ·ä¼ é€’ç»™ä½ çš„è¯„ä¼°å™¨ã€‚\n",
    "\n",
    "è¯„ä¼°å™¨æ‰§è¡Œä½ å®šä¹‰çš„é€»è¾‘ï¼ˆæ¯”å¦‚æ¯”è¾ƒä¸¤è€…æ˜¯å¦å®Œå…¨åŒ¹é…ï¼‰ï¼Œå¹¶è¿”å›ä¸€ä¸ªåˆ†æ•°æˆ–è¯„ä¼°ç»“æœã€‚\n",
    "\n",
    "æ‰€æœ‰è¿™äº›ç»“æœï¼ˆåŒ…æ‹¬è¿è¡Œè®°å½•ã€è¾“å…¥ã€è¾“å‡ºå’Œè¯„ä¼°åˆ†æ•°ï¼‰éƒ½ä¼šè¢«è‡ªåŠ¨è®°å½•åœ¨ LangSmith å¹³å°ä¸Šï¼Œæ–¹ä¾¿ä½ è¿›è¡ŒæŸ¥çœ‹å’Œåˆ†æã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d3f232",
   "metadata": {},
   "source": [
    "å°†åˆ›å»ºçš„ LangSmith æ•°æ®é›†ä¼ é€’ç»™â€œé‚®ä»¶åŠ©æ‰‹â€ï¼ˆemail assistantï¼‰ï¼Œè¯„ä¼°å®ƒçš„triage_routerèƒ½åŠ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b9d1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_email_assistant(inputs: dict) -> dict:\n",
    "    \"\"\"Process an email through the workflow-based email assistant.\"\"\"\n",
    "    response = email_assistant.nodes['triage_router'].invoke({\"email_input\": inputs[\"email_input\"]})\n",
    "    return {\"classification_decision\": response.update['classification_decision']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6ec4c",
   "metadata": {},
   "source": [
    "#### Evaluator Function \n",
    "\n",
    "Now, we create an evaluator function. What do we want to evaluate? We have reference outputs in our dataset and agent outputs defined in the functions above.\n",
    "\n",
    "* Reference outputs: `\"reference_outputs\": {\"classification\": triage_output_1} ...`\n",
    "* Agent outputs: `\"outputs\": {\"classification_decision\": agent_output_1} ...`\n",
    "\n",
    "We want to evaluate if the agent's output matches the reference output. So we simply need a an evaluator function that compares the two, where `outputs` is the agent's output and `reference_outputs` is the reference output from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d84a46c",
   "metadata": {},
   "source": [
    "æ¯”è¾ƒagentçš„åˆ†ç±»ä¸æœŸæœ›çš„åˆ†ç±»æ˜¯å¦ä¸€è‡´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fee7532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_evaluator(outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"Check if the answer exactly matches the expected answer.\"\"\"\n",
    "    return outputs[\"classification_decision\"].lower() == reference_outputs[\"classification\"].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fd2de9",
   "metadata": {},
   "source": [
    "### Running Evaluation\n",
    "\n",
    "Now, the question is: how are these things hooked together? The evaluate API takes care of it for us. It passes the `inputs` dict from our dataset the target function. It passes the `reference_outputs` dict from our dataset to the evaluator function. And it passes the `outputs` of our agent to the evaluator function. \n",
    "\n",
    "Note this is similar to what we did with Pytest: in Pytest, we passed in the dataset example inputs and reference outputs to the test function with `@pytest.mark.parametrize`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e355978",
   "metadata": {},
   "source": [
    "è°ƒç”¨client.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6807306d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'E-mail assistant workflow-14d57ff3' at:\n",
      "https://smith.langchain.com/o/f8ee6d35-d073-4b10-97a0-06b3dbd45cab/datasets/a71f675c-20d3-4801-8a48-46b1cca493f3/compare?selectedSessions=618f9c9a-0072-497b-b10b-209abbeef904\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76db636f3a64dd098944342c0582a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”” Classification: NOTIFY - This email contains important information\n",
      "ğŸ“§ Classification: RESPOND - This email requires a response\n",
      "ğŸ”” Classification: NOTIFY - This email contains important information\n",
      "ğŸ”” Classification: NOTIFY - This email contains important information\n",
      "ğŸš« Classification: IGNORE - This email can be safely ignored\n",
      "ğŸš« Classification: IGNORE - This email can be safely ignored\n",
      "ğŸš« Classification: IGNORE - This email can be safely ignored\n",
      "ğŸ“§ Classification: RESPOND - This email requires a response\n",
      "ğŸ“§ Classification: RESPOND - This email requires a response\n",
      "ğŸ“§ Classification: RESPOND - This email requires a response\n",
      "ğŸ”” Classification: NOTIFY - This email contains important information\n",
      "ğŸ“§ Classification: RESPOND - This email requires a response\n",
      "ğŸ“§ Classification: RESPOND - This email requires a response\n",
      "ğŸ“§ Classification: RESPOND - This email requires a response\n",
      "ğŸ“§ Classification: RESPOND - This email requires a response\n",
      "ğŸ”” Classification: NOTIFY - This email contains important information\n"
     ]
    }
   ],
   "source": [
    "# Set to true if you want to kick off evaluation\n",
    "run_expt = True\n",
    "if run_expt:\n",
    "    experiment_results_workflow = client.evaluate(\n",
    "        # Run agent \n",
    "        target_email_assistant,\n",
    "        # Dataset name   \n",
    "        data=dataset_name,\n",
    "        # Evaluator\n",
    "        evaluators=[classification_evaluator],\n",
    "        # Name of the experiment\n",
    "        experiment_prefix=\"E-mail assistant workflow\", \n",
    "        # Number of concurrent evaluations\n",
    "        max_concurrency=2, \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76baff88",
   "metadata": {},
   "source": [
    "We can view the results from both experiments in the LangSmith UI.\n",
    "\n",
    "![Test Results](img/eval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5146b52",
   "metadata": {},
   "source": [
    "## LLM-as-Judge Evaluation\n",
    "\n",
    "We've shown unit tests for the triage step (using evaluate()) and tool calling (using Pytest). \n",
    "\n",
    "We'll showcase how you could use an LLM as a judge to evaluate our agent's execution against a set of success criteria. \n",
    "\n",
    "![types](img/eval_types.png)\n",
    "\n",
    "First, we define a structured output schema for our LLM grader that contains a grade and justification for the grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1d342b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "class CriteriaGrade(BaseModel):\n",
    "    \"\"\"Score the response against specific criteria.\"\"\"\n",
    "    justification: str = Field(description=\"The justification for the grade and score, including specific examples from the response.\")\n",
    "    grade: bool = Field(description=\"Does the response meet the provided criteria?\")\n",
    "    \n",
    "# Create a global LLM for evaluation to avoid recreating it for each test\n",
    "criteria_eval_llm = init_chat_model(\"gpt-4o-mini\")\n",
    "criteria_eval_structured_llm = criteria_eval_llm.with_structured_output(CriteriaGrade)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade03e9",
   "metadata": {},
   "source": [
    "email_inputs[0]å’Œresponse_criteria_list[0]åœ¨æ–‡ä»¶src\\email_assistant\\eval\\email_dataset.pyä¸­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bec02b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email Input: {'author': 'Alice Smith <alice.smith@company.com>', 'to': 'Lance Martin <lance@company.com>', 'subject': 'Quick question about API documentation', 'email_thread': \"Hi Lance,\\n\\nI was reviewing the API documentation for the new authentication service and noticed a few endpoints seem to be missing from the specs. Could you help clarify if this was intentional or if we should update the docs?\\n\\nSpecifically, I'm looking at:\\n- /auth/refresh\\n- /auth/validate\\n\\nThanks!\\nAlice\"}\n",
      "Success Criteria: \n",
      "â€¢ Send email with write_email tool call to acknowledge the question and confirm it will be investigated  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "email_input = email_inputs[0]\n",
    "print(\"Email Input:\", email_input)\n",
    "success_criteria = response_criteria_list[0]\n",
    "print(\"Success Criteria:\", success_criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38390ccd",
   "metadata": {},
   "source": [
    "Our Email Assistant is invoked with the email input and the response is formatted into a string. These are all then passed to the LLM grader to receive a grade and justification for the grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbff28fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“§ Classification: RESPOND - This email requires a response\n"
     ]
    }
   ],
   "source": [
    "response = email_assistant.invoke({\"email_input\": email_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d64619fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CriteriaGrade(justification='The response satisfies the criterion by using the write_email tool call to acknowledge Alice\\'s question and confirm that the issue will be investigated. Specifically, the content of the email states: \"Thank you for bringing this to my attention. I\\'ll investigate whether the /auth/refresh and /auth/validate endpoints were intentionally omitted from the API documentation or if an update is needed.\" This directly acknowledges the question and confirms an investigation, fulfilling the required criterion.', grade=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from email_assistant.eval.prompts import RESPONSE_CRITERIA_SYSTEM_PROMPT\n",
    "\n",
    "all_messages_str = format_messages_string(response['messages'])\n",
    "eval_result = criteria_eval_structured_llm.invoke([\n",
    "        {\"role\": \"system\",\n",
    "            \"content\": RESPONSE_CRITERIA_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",\n",
    "            \"content\": f\"\"\"\\n\\n Response criteria: {success_criteria} \\n\\n Assistant's response: \\n\\n {all_messages_str} \\n\\n Evaluate whether the assistant's response meets the criteria and provide justification for your evaluation.\"\"\"}\n",
    "    ])\n",
    "\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64275647-6fdb-4bf3-806b-4dbc770cbd6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are evaluating an email assistant that works on behalf of a user.\\n\\nYou will see a sequence of messages, starting with an email sent to the user. \\n\\nYou will then see the assistant's response to this email on behalf of the user, which includes any tool calls made (e.g., write_email, schedule_meeting, check_calendar_availability, done).\\n\\nYou will also see a list of criteria that the assistant's response must meet.\\n\\nYour job is to evaluate if the assistant's response meets ALL the criteria bullet points provided.\\n\\nIMPORTANT EVALUATION INSTRUCTIONS:\\n1. The assistant's response is formatted as a list of messages.\\n2. The response criteria are formatted as bullet points (â€¢)\\n3. You must evaluate the response against EACH bullet point individually\\n4. ALL bullet points must be met for the response to receive a 'True' grade\\n5. For each bullet point, cite specific text from the response that satisfies or fails to satisfy it\\n6. Be objective and rigorous in your evaluation\\n7. In your justification, clearly indicate which criteria were met and which were not\\n7. If ANY criteria are not met, the overall grade must be 'False'\\n\\nYour output will be used for automated testing, so maintain a consistent evaluation approach.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESPONSE_CRITERIA_SYSTEM_PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7994952c",
   "metadata": {},
   "source": [
    "We can see that the LLM grader returns an eval result with a schema matching our `CriteriaGrade` base model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b44111d",
   "metadata": {},
   "source": [
    "## Running against a Larger Test Suite\n",
    "Now that we've seen how to evaluate our agent using Pytest and evaluate(), and seen an example of using an LLM as a judge, we can use evaluations over a bigger test suite to get a better sense of how our agent performs over a wider variety of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9280d5ae-3070-4131-8763-454073176081",
   "metadata": {},
   "source": [
    "Let's run our email_assistant against a larger test suite.\n",
    "```\n",
    "! LANGSMITH_TEST_SUITE='Email assistant: Test Full Response Interrupt' LANGSMITH_EXPERIMENT='email_assistant' pytest tests/test_response.py --agent-module email_assistant\n",
    "```\n",
    "\n",
    "In `test_response.py`, you can see a few things. \n",
    "\n",
    "We pass our dataset examples into functions that will run pytest and log to our `LANGSMITH_TEST_SUITE`:\n",
    "\n",
    "```\n",
    "# Reference output key\n",
    "@pytest.mark.langsmith(output_keys=[\"criteria\"])\n",
    "# Variable names and a list of tuples with the test cases\n",
    "# Each test case is (email_input, email_name, criteria, expected_calls)\n",
    "@pytest.mark.parametrize(\"email_input,email_name,criteria,expected_calls\",create_response_test_cases())\n",
    "def test_response_criteria_evaluation(email_input, email_name, criteria, expected_calls):\n",
    "```\n",
    "\n",
    "We use LLM-as-judge with a grading schema:\n",
    "```\n",
    "class CriteriaGrade(BaseModel):\n",
    "    \"\"\"Score the response against specific criteria.\"\"\"\n",
    "    grade: bool = Field(description=\"Does the response meet the provided criteria?\")\n",
    "    justification: str = Field(description=\"The justification for the grade and score, including specific examples from the response.\")\n",
    "```\n",
    "\n",
    "\n",
    "We evaluate the agent response relative to the criteria:\n",
    "```\n",
    "    # Evaluate against criteria\n",
    "    eval_result = criteria_eval_structured_llm.invoke([\n",
    "        {\"role\": \"system\",\n",
    "            \"content\": RESPONSE_CRITERIA_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",\n",
    "            \"content\": f\"\"\"\\n\\n Response criteria: {criteria} \\n\\n Assistant's response: \\n\\n {all_messages_str} \\n\\n Evaluate whether the assistant's response meets the criteria and provide justification for your evaluation.\"\"\"}\n",
    "    ])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca836fbf",
   "metadata": {},
   "source": [
    "Now let's take a look at this experiment in the LangSmith UI and look into what our agent did well, and what it could improve on.\n",
    "\n",
    "#### Getting Results\n",
    "\n",
    "We can also get the results of the evaluation by reading the tracing project associated with our experiment. This is great for creating custom visualizations of our agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70b655f8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# TODO: Copy your experiment name here\n",
    "experiment_name = \"email_assistant:8286b3b8\"\n",
    "# Set this to load expt results\n",
    "load_expt = False\n",
    "if load_expt:\n",
    "    email_assistant_experiment_results = client.read_project(project_name=experiment_name, include_stats=True)\n",
    "    print(\"Latency p50:\", email_assistant_experiment_results.latency_p50)\n",
    "    print(\"Latency p99:\", email_assistant_experiment_results.latency_p99)\n",
    "    print(\"Token Usage:\", email_assistant_experiment_results.total_tokens)\n",
    "    print(\"Feedback Stats:\", email_assistant_experiment_results.feedback_stats)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "email_assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
